# 8. ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ë¡¤ë§ (instagram crawling)

- data ë‹¤ìš´ë¡œë“œ: https://github.com/Play-with-data/datasalon/tree/master/02_%EA%B0%9C%EC%A0%95%ED%8C%90/5_Jeju_Hotplace

## 8.1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°

```python
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup

import pandas as pd
import numpy as np
import time
```



## 8.2. ì¸ìŠ¤íƒ€ê·¸ë¨ ì ‘ì† í›„ ë¡œê·¸ì¸ í•˜ê¸°

```python
ser = Service('../chromedriver/chromedriver.exe')
driver = webdriver.Chrome(service = ser)

url = 'https://www.instagram.com/'
driver.get(url)
time.sleep(3)
```

- ì•„ë˜ ë‚´ìš©ì€ ìŠ¤í‚µí•˜ê³  ì§ì ‘ ë¡œê·¸ì¸ í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.

```python
# ì¸ìŠ¤íƒ€ ê³„ì • ë¡œê·¸ì¸í•˜ê¸°
# ì›ì¹˜ì•Šë‹¤ë©´ ì§ì ‘ ë¸Œë¼ìš°ì €ì— ê³„ì • ì •ë³´ë¥¼ ì…ë ¥í•´ë„ ë˜ê³ , ì½”ë“œë¥¼ ì´ìš©í•´ë„ ë©ë‹ˆë‹¤.
# ë‹¤ë§Œ, ë³¸ì¸ì˜ ê³„ì •/ë¹„ë²ˆ ì •ë³´ê°€ ì™¸ë¶€ì— ë…¸ì¶œë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
# ì €ëŠ” ì§ì ‘ ë¡œê·¸ì¸ í–ˆìŠµë‹ˆë‹¤.
email = 'ì¸ìŠ¤íƒ€ê³„ì •ì…ë ¥í•´ì£¼ì„¸ìš”'   ### ê³„ì • ì •ë³´ ìˆ˜ì • í•„ìš”
input_id = driver.find_elements_by_css_selector('input._2hvTZ.pexuQ.zyHYP')[0]
input_id.clear()
input_id.send_keys(email)

password = 'ì¸ìŠ¤íƒ€ë¹„ë²ˆì…ë ¥í•´ì£¼ì„¸ìš”' ### ë¹„ë²ˆ ì •ë³´ ìˆ˜ì • í•„ìš”
input_pw = driver.find_elements_by_css_selector('input._2hvTZ.pexuQ.zyHYP')[1]
input_pw.clear()
input_pw.send_keys(password)
input_pw.submit()
time.sleep(3)
```



## 8.3. ì¸ìŠ¤íƒ€ê·¸ë¨ ê²€ìƒ‰ê²°ê³¼ URL ë§Œë“¤ì–´ì„œ ì ‘ì†

```python
def insta_searching(word):
    url = 'https://www.instagram.com/explore/tags/{}/'.format(word)
#     url = 'https://www.instagram.com/explore/tags/' + word # ì´ê²ƒë„ ê°€ëŠ¥
    return(url)
```



## 8.4. ê²€ìƒ‰ê²°ê³¼ í˜ì´ì§€ ì ‘ì†í•˜ê¸°

```python
word = 'ì œì£¼ë„ë§›ì§‘'
url = insta_searching(word)
driver.get(url)
time.sleep(3) # í˜ì´ì§€ê°€ ì§„í–‰ë˜ê¸°ì „ì— page_sourceë¥¼ ê°€ì ¸ì˜¤ë©´ ì˜¤ë¥˜ê°€ ë‚˜ì™€ì„œ delayë¥¼ ì¤Œ
```



## 8.5. í™”ë©´ ì²«ë²ˆì§¸ ê²Œì‹œê¸€ ì„ íƒí•˜ê¸°

```python
from selenium.webdriver.common.by import By

def select_first(driver):
    # ì›¹í˜ì´ì§€ì— ì ‘ì†í•´ì„œ ì²«ë²ˆì§¸ ê²Œì‹œê¸€ í•´ë‹¹í•˜ëŠ” tag ì„ íƒí•´ì¤˜
    first = driver.find_element(By.CSS_SELECTOR, 'div._9AhH0') 
    first.click()
    time.sleep(3)

select_first(driver)
```



## 8.6. html ì •ë³´ ê°€ì ¸ì˜¤ê¸°

```python
# ê°€ì ¸ì˜¬ ì •ë³´ ['content','data','like','place','tags']
html_insta = driver.page_source

soup = BeautifulSoup(html_insta, 'html.parser')
soup
'''
ìƒëµ
'''
```

#### 8.6.1. content

```python
import unicodedata # macì—ì„œ ì‘ì„±í•œ ê¸€ ê¹¨ì§€ì§€ ì•Šê²Œ

content = soup.select('div.ZyFrc div.C4VMK > span')[0].text
content = unicodedata.normalize('NFC', content) # ê¹¨ì§€ëŠ” ê¸€ì”¨ ìœ„í•´

content
'''
'<ê´‘ê³ >ì˜¨ê°€ì¡±ì´ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì œì£¼ê°ˆë¹„êµ­ìˆ˜!!ğŸ’•âœ”ì†ì†Œë…ì œ í•­ì‹œ ë°°ì¹˜(ì…ì¥ì‹œ ì…êµ¬ë¶€í„° ì†ì†Œë…ì œ ë§ˆìŠ¤í¬ ì°©ìš©í›„ ì…ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤ğŸ¥°)â–ªï¸ì˜¤í”ˆì‹œê°„: 07:30 - 17:00â–ªï¸ì „í™”: 064-782-5105â–ªï¸ì œì£¼ì‹œ êµ¬ì¢Œì í•´ë§ì´í•´ì•ˆë¡œ 2284â–ªï¸ë§¤ì£¼ í™”ìš”ì¼ íœ´ë¬´ì‚°ë„ë¡±ë§¨ë„ë¡±#ì œì£¼ë§›ì§‘ #ì œì£¼ë„ë§›ì§‘ #ì œì£¼ì•„ì¿ ì•„í”Œë¼ë„·#í‘œì„ ë§›ì§‘ #ì„±ì‚°ë§›ì§‘ #ì„±ì‚°ì¼ì¶œë´‰ë§›ì§‘ #êµ¬ì¢Œìë§›ì§‘ #êµ¬ì¢Œë§›ì§‘ #ë¹„ìë¦¼ë§›ì§‘ #ì•ˆëŒì˜¤ë¦„ #ê´‘ì¹˜ê¸°í•´ë³€ë§›ì§‘ #ì œì£¼ê³ ê¸°êµ­ìˆ˜#ì œì£¼ë„ê³ ê¸°êµ­ìˆ˜ #ì„±ì‚°ì•„ì¹¨ì‹ì‚¬'
'''
```

#### 8.6.2. tags

##### 8.6.2.1. ì •ê·œì‹ í™•ì¸í•˜ê¸°

- https://wikidocs.net/4308

```python
import re

print('hello\n')
print('hello')
print(r'hello\n')
print('hello')
# r ì€ ê¸€ì ê·¸ë˜ë¡œ ì¸ì‹í•˜ê²Œ í•´ì¤Œ
'''
hello

hello
hello\n
hello
'''
```

##### 8.6.2.2. tags ê°€ì ¸ì˜¤ê¸°

```python
tags = re.findall(r'#[^\s#,\\]+',content)
tags
'''
['#ì œì£¼ë§›ì§‘',
 '#ì œì£¼ë„ë§›ì§‘',
 '#ì œì£¼ì•„ì¿ ì•„í”Œë¼ë„·',
 '#í‘œì„ ë§›ì§‘',
 '#ì„±ì‚°ë§›ì§‘',
 '#ì„±ì‚°ì¼ì¶œë´‰ë§›ì§‘',
 '#êµ¬ì¢Œìë§›ì§‘',
 '#êµ¬ì¢Œë§›ì§‘',
 '#ë¹„ìë¦¼ë§›ì§‘',
 '#ì•ˆëŒì˜¤ë¦„',
 '#ê´‘ì¹˜ê¸°í•´ë³€ë§›ì§‘',
 '#ì œì£¼ê³ ê¸°êµ­ìˆ˜',
 '#ì œì£¼ë„ê³ ê¸°êµ­ìˆ˜',
 '#ì„±ì‚°ì•„ì¹¨ì‹ì‚¬']
'''
```



#### 8.6.3. date

```python
date = soup.select('time._1o9PC.Nzb55')[0]['datetime'][:10]
type(date) # str
date
'''
'2022-01-16'
'''
```



#### 8.6.4. likes

```python
likes = soup.select('a.zV_Nj > span')
```



#### 8.6.5. place

```python
place = soup.select('a.O4GlU')[0].text
place = unicodedata.normalize('NFC', content) # ê¹¨ì§€ëŠ” ê¸€ì”¨ ìœ„í•´
```



## 8.7. html ì •ë³´ ëª¨ì•„ì˜¤ê¸° def ë§Œë“¤ê¸°

```python
def get_content(driver):
    html_insta = driver.page_source
    soup = BeautifulSoup(html_insta, 'html.parser')
    
    try:
        content = soup.select('div.ZyFrc div.C4VMK > span')[0].text
        content = unicodedata.normalize('NFC', content) # ê¹¨ì§€ëŠ” ê¸€ì”¨ ìœ„í•´
    except:
        content = " "
    
    try:
        tags = re.findall(r'#[^\s#,\\]+',content)
    except:
        tags = " "
    
    date = soup.select('time._1o9PC.Nzb55')[0]['datetime'][:10]
    
    try:
        likes = soup.select('a.zV_Nj > span')[0].text
    except:
        likes = 0
        
    try:
        place = soup.select('a.O4GlU')[0].text
        place = unicodedata.normalize('NFC', content) # ê¹¨ì§€ëŠ” ê¸€ì”¨ ìœ„í•´
    except:
        place = ""
    
    data = [content, date, likes, place, tags]
    
    return(data)
```
- í™•ì¸í•´ë³´ê¸°

```python
mylist = get_content(driver)
mylist
'''
['<ê´‘ê³ >ì˜¨ê°€ì¡±ì´ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì œì£¼ê°ˆë¹„êµ­ìˆ˜!!ğŸ’•âœ”ì†ì†Œë…ì œ í•­ì‹œ ë°°ì¹˜(ì…ì¥ì‹œ ì…êµ¬ë¶€í„° ì†ì†Œë…ì œ ë§ˆìŠ¤í¬ ì°©ìš©í›„ ì…ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤ğŸ¥°)â–ªï¸ì˜¤í”ˆì‹œê°„: 07:30 - 17:00â–ªï¸ì „í™”: 064-782-5105â–ªï¸ì œì£¼ì‹œ êµ¬ì¢Œì í•´ë§ì´í•´ì•ˆë¡œ 2284â–ªï¸ë§¤ì£¼ í™”ìš”ì¼ íœ´ë¬´ì‚°ë„ë¡±ë§¨ë„ë¡±#ì œì£¼ë§›ì§‘ #ì œì£¼ë„ë§›ì§‘ #ì œì£¼ì•„ì¿ ì•„í”Œë¼ë„·#í‘œì„ ë§›ì§‘ #ì„±ì‚°ë§›ì§‘ #ì„±ì‚°ì¼ì¶œë´‰ë§›ì§‘ #êµ¬ì¢Œìë§›ì§‘ #êµ¬ì¢Œë§›ì§‘ #ë¹„ìë¦¼ë§›ì§‘ #ì•ˆëŒì˜¤ë¦„ #ê´‘ì¹˜ê¸°í•´ë³€ë§›ì§‘ #ì œì£¼ê³ ê¸°êµ­ìˆ˜#ì œì£¼ë„ê³ ê¸°êµ­ìˆ˜ #ì„±ì‚°ì•„ì¹¨ì‹ì‚¬',
 '2022-01-16',
 0,
 '',
 ['#ì œì£¼ë§›ì§‘',
  '#ì œì£¼ë„ë§›ì§‘',
  '#ì œì£¼ì•„ì¿ ì•„í”Œë¼ë„·',
  '#í‘œì„ ë§›ì§‘',
  '#ì„±ì‚°ë§›ì§‘',
  '#ì„±ì‚°ì¼ì¶œë´‰ë§›ì§‘',
  '#êµ¬ì¢Œìë§›ì§‘',
  '#êµ¬ì¢Œë§›ì§‘',
  '#ë¹„ìë¦¼ë§›ì§‘',
  '#ì•ˆëŒì˜¤ë¦„',
  '#ê´‘ì¹˜ê¸°í•´ë³€ë§›ì§‘',
  '#ì œì£¼ê³ ê¸°êµ­ìˆ˜',
  '#ì œì£¼ë„ê³ ê¸°êµ­ìˆ˜',
  '#ì„±ì‚°ì•„ì¹¨ì‹ì‚¬']]
'''
```



## 8.8. crawling def ë§Œë“¤ê¸°

```python


def insta_crawling(word, n):
    
    url = insta_searching(word)
    driver.get(url)
    time.sleep(8) # í˜ì´ì§€ê°€ ì§„í–‰ë˜ê¸°ì „ì— pagesourceë¥¼ ê°€ì ¸ì˜¤ë©´ ì˜¤ë¥˜ê°€ ë‚˜ì™€ì„œ delayë¥¼ ì¤Œ

    
    # ì²« ì‚¬ì§„ ì„ íƒ
    select_first(driver)
    time.sleep(8)
    
   
    
    # ë‹¤ìŒ ë„˜ì–´ê°€ê¸°
	# target = n
    # n = í¬ë¡¤ë§í•  ê²Œì‹œê¸€ ìˆ˜
    results = []
    # n ëŒ€ì‹  targetì„ ë„£ì–´ì¤˜ë„ ë¨
    for i in range(n):
        try:
             # contents ìˆ˜ì§‘
            data = get_content(driver)
            results.append(data)
            move_next(driver)
            
        except:
            time.sleep(2)
            move_next(driver)
    
    return(results)
```

## 8.9. ë°ì´í„° ì €ì¥

```python
results_df = pd.DataFrame(results)
results_df.columns = ['content','data','like','place','tags']
results_df.to_excel('./files/1_crawling_jejudoMatJip.xlsx')
```

- ìœ„ ë°©ë²•ìœ¼ë¡œ ì—¬ëŸ¬ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë³´ì





## 8.10. ì—¬ëŸ¬ ë°ì´í„° í†µí•©

```python
jeju_insta_df = pd.DataFrame( [ ] )

folder = './files/'
f_list = ['1_crawling_jejudoMatJip.xlsx', '1_crawling_jejudoGwanGwang.xlsx', '1_crawling_jejuMatJip.xlsx', '1_crawling_jejuYeoHang.xlsx']
for fname in f_list:
    fpath = folder + fname
    temp = pd.read_excel(fpath)
    jeju_insta_df = jeju_insta_df.append(temp)

jeju_insta_df.columns =['content','data','like','place','tags']

# ì¤‘ë³µ ë°ì´í„° ì œê±°í•˜ê³  ì €ì¥
jeju_insta_df.drop_duplicates(subset = [ "content"] , inplace = True)
jeju_insta_df.to_excel('./files/1_crawling_raw.xlsx', index = False)
```

